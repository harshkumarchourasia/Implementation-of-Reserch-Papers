{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout2d\n",
    "from torch.optim import Adam, SGD\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from skimage.transform import rotate, AffineTransform, warp\n",
    "import skimage.io as io\n",
    "import ctypes\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import rotate\n",
    "from random import random\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaing the dataset which was made in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load(\"X_50.npy\")\n",
    "labels = np.load(\"Y_50.npy\")\n",
    "images = images.reshape((images.shape[0],1,50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataapoint in each class is  [188469, 35755, 70384, 48755]\n"
     ]
    }
   ],
   "source": [
    "v=[0,0,0,0]\n",
    "for i in labels:\n",
    "    v[i]+=1\n",
    "print(\"number of dataapoint in each class is \", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of data poinsts 343363\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of data poinsts\", images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset just loaded is highly unbalanced. So during augmentation we augment less occuring class more frequently and more occuring class less frequently so that it can become balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1701.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# vector X and Y contains all the training images and labels of the augmentatied dataset\n",
    "train_img = []\n",
    "Y = []\n",
    "for itr in tqdm(range(len(labels))):\n",
    "    p ={'0':[.25,.25,.25,.25],\n",
    "        '1':[1,1,1,1],\n",
    "        '2':[.7,.7,.7,.7],\n",
    "        '3':[1,1,1,1]}\n",
    "    l = str(labels[itr])\n",
    "    pr = random()\n",
    "    if pr < p[l][0]:\n",
    "        train_img.append(images[itr,:,:])\n",
    "        Y.append(labels[itr])\n",
    "\n",
    "    pr = random()\n",
    "    if pr<p[l][1]:\n",
    "        train_img.append(cv2.rotate(images[itr,:,:], cv2.ROTATE_180))\n",
    "        Y.append(labels[itr])\n",
    "    pr = random()\n",
    "    if pr<p[l][2]:\n",
    "        temp=[]\n",
    "        temp.append(rotate(images[itr,:,:][0],90))\n",
    "        train_img.append(temp)\n",
    "        Y.append(labels[itr])\n",
    "        \n",
    "    pr = random()\n",
    "    if pr<p[l][3]:\n",
    "        temp=[]\n",
    "        temp.append(rotate(images[itr,:,:][0],270))\n",
    "        train_img.append(temp)\n",
    "        Y.append(labels[itr])    \n",
    "train_img = np.array(train_img)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of dataapoint in each class are  [10, 0, 11, 0]\n"
     ]
    }
   ],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_img,Y, test_size = 0.3, shuffle = True)\n",
    "(train_x.shape, train_y.shape), (val_x.shape, val_y.shape)\n",
    "v=[0,0,0,0]\n",
    "for i in Y:\n",
    "    v[i]+=1\n",
    "print(\"number of dataapoint in each class are \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is approximately balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.astype(int)\n",
    "val_y = val_y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These classes helps to load data and implement batch wise training\n",
    "class TrainHelper(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = train_x.shape[0]\n",
    "        self.train_x, self.train_y = train_x.astype('float32'), train_y.astype('float32')\n",
    "        self.train_x = torch.from_numpy(train_x)\n",
    "        self.train_y = torch.from_numpy(train_y)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_x[idx], self.train_y[idx]\n",
    "class ValHelper(Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = val_x.shape[0]\n",
    "        self.val_x, self.val_y = val_x.astype('float32'), val_y.astype('float32')\n",
    "        self.val_x = torch.from_numpy(val_x)\n",
    "        self.val_y = torch.from_numpy(val_y)\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, idx):\n",
    "        return self.val_x[idx], self.val_y[idx]\n",
    "traindataset = TrainHelper()\n",
    "valdataset = ValHelper()\n",
    "train_loader = DataLoader(dataset = traindataset, batch_size=32,shuffle=True,num_workers=0)\n",
    "val_loader = DataLoader(dataset = valdataset, batch_size=32,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 64, kernel_size=10, stride=1, padding=0),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(64),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout2d(0.10),\n",
    "            \n",
    "            Conv2d(128, 128, kernel_size=2, stride=1, padding=2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),         \n",
    "            Dropout2d(0.20),\n",
    "            \n",
    "            Conv2d(128,6272, kernel_size=7, padding=0),   # This technique is called fully connected layer using convulation layer.\n",
    "            ReLU(inplace=True),\n",
    "            \n",
    "            Conv2d(6272, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x\n",
    "class Net2(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 64, kernel_size=10, stride=1, padding=0),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(64),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=3, stride=1, padding=2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout2d(0.10),\n",
    "            \n",
    "            Conv2d(128, 256, kernel_size=2, stride=1, padding=2),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),         \n",
    "            Dropout2d(0.20),\n",
    "            \n",
    "            Conv2d(256,12544, kernel_size=7, padding=0),   # This tenique is called fully connected layer using convulation layer.\n",
    "            ReLU(inplace=True),\n",
    "            \n",
    "            Conv2d(12544, 4, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 4, 1, 1]             --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 41, 41]          6,464\n",
      "|    └─ReLU: 2-2                         [-1, 64, 41, 41]          --\n",
      "|    └─BatchNorm2d: 2-3                  [-1, 64, 41, 41]          128\n",
      "|    └─MaxPool2d: 2-4                    [-1, 64, 20, 20]          --\n",
      "|    └─Conv2d: 2-5                       [-1, 128, 22, 22]         73,856\n",
      "|    └─ReLU: 2-6                         [-1, 128, 22, 22]         --\n",
      "|    └─MaxPool2d: 2-7                    [-1, 128, 11, 11]         --\n",
      "|    └─Dropout2d: 2-8                    [-1, 128, 11, 11]         --\n",
      "|    └─Conv2d: 2-9                       [-1, 256, 14, 14]         131,328\n",
      "|    └─ReLU: 2-10                        [-1, 256, 14, 14]         --\n",
      "|    └─MaxPool2d: 2-11                   [-1, 256, 7, 7]           --\n",
      "|    └─Dropout2d: 2-12                   [-1, 256, 7, 7]           --\n",
      "|    └─Conv2d: 2-13                      [-1, 12544, 1, 1]         157,364,480\n",
      "|    └─ReLU: 2-14                        [-1, 12544, 1, 1]         --\n",
      "|    └─Conv2d: 2-15                      [-1, 4, 1, 1]             50,180\n",
      "==========================================================================================\n",
      "Total params: 157,626,436\n",
      "Trainable params: 157,626,436\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 387.15\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 2.59\n",
      "Params size (MB): 601.30\n",
      "Estimated Total Size (MB): 603.90\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr=np.random.randn(1,1,50,50)\n",
    "arr=torch.from_numpy(arr).float()\n",
    "model = Net2()\n",
    "summary(model,(1,50,50))\n",
    "model(arr).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net2()\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#Stochastic Gradient Descent is used\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1, verbose=True)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr1=0\n",
    "highest_validation_accuracy = 0\n",
    "def train(epoch,tb):\n",
    "    global highest_validation_accuracy\n",
    "    global ctr1\n",
    "    # ctr1 are used to count the epoch for the test and training mini-batches\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "        data, target = batch\n",
    "        x_train, y_train = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x_train = x_train.cuda()\n",
    "            y_train = y_train.cuda()\n",
    "        \n",
    "        output_train = model(x_train.float())\n",
    "        output_train = output_train.reshape(-1,4)\n",
    "        loss_train = criterion(output_train, y_train)\n",
    "        train_loss.append(loss_train.item()) \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        ctr1+=1\n",
    "        \n",
    "        _, predicted = torch.max(output_train.data, 1)\n",
    "        total += y_train.size(0)\n",
    "        correct += (predicted == y_train).sum().item()\n",
    "        if ctr1%50==0:\n",
    "            print(\"Training accuracy\", correct/total)     \n",
    "    tb.add_scalar(\"Training Loss\", loss_train.item(), ctr1)\n",
    "    tb.add_scalar(\"Training accuracy\", correct/total,ctr1)     \n",
    "    \n",
    "\n",
    "    print(\"Training Loss on loop\",ctr1,\" is \",  np.mean(train_loss))\n",
    "    print(\"Training accuracy on loop\",ctr1,\" is \", correct/total)\n",
    "    validation_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in val_loader:\n",
    "        data, target = batch\n",
    "        x_val, y_val = Variable(data), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "        output_val = model(x_val.float())\n",
    "        output_val =  output_val.reshape(-1,4)\n",
    "        loss_val = criterion(output_val, y_val)\n",
    "        validation_loss.append(loss_val.item())            \n",
    "        _, predicted = torch.max(output_val.data, 1)\n",
    "        total += y_val.size(0)\n",
    "        correct += (predicted == y_val).sum().item()\n",
    "    tb.add_scalar(\"Validation Loss\", np.mean(validation_loss), ctr1)\n",
    "    tb.add_scalar(\"Validation accuracy\", correct/total,ctr1)\n",
    "    print(\"Validation Loss on loop\",ctr1,\" is \",  np.mean(validation_loss))\n",
    "    print(\"Validation accuracy on loop\",ctr1,\" is \", correct/total)\n",
    "    if highest_validation_accuracy-0.005 < correct/total:\n",
    "            highest_validation_accuracy = correct/total\n",
    "            name = \"ValidationAccuracy\"+str(highest_validation_accuracy)+\"epoch\"+str(ctr1)+'.pth'\n",
    "            torch.save(model.state_dict(),name)\n",
    "    \n",
    "    scheduler.step(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "tb = SummaryWriter(comment = 'second model')\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch,tb)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
